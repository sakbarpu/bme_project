Project Description;;;

	The goal of this repo is to implement word2vec from scratch in python
	without using any high-level machine learning libraries such as 
	Pytorch, or Tensorflow, or scikit-learn.

	There are, however, many python libraries imported to implement word2vec
	which perform low level operations on numbers, arrays, and strings.

	The project implements word2vec skipgram model.
	
	Mikolov et al., "Distributed representations of words and phrases and their compositionality", NIPS 2013.

	We have used word2vec based word embeddings for software search in SCOR
	source code retrieval engine.
	https://engineering.purdue.edu/RVL/SCOR_WordEmbeddings/

Libraries required;;;

	sys, os, time
	argparse
	fnmatch
	nltk #only used for stemming (a preprocessing step for text data which may be ignored)
	regex
	string
	random
	itertools
	multiprocessing as mp
	math
	warnings
	pickle
	numpy as np
	scipy.special # for expit function


How to use the scripts;;;

	usage: word2vec.py [-h] [-corpuspath corpus_dir]
			   [-processedfilepath processed_file_path] -output output_dir

	optional arguments:
	  -h, --help            show this help message and exit
	  -corpuspath corpus_dir, --corpuspath corpus_dir
				which directory is containing corpus (e.g. eclipse
				path)
	  -processedfilepath processed_file_path, --processedfilepath processed_file_path
				which directory is containing the preprocessed file
				(each document in a separate line)
	  -output output_dir, --output output_dir
				which directory is output

	
	The script can either be provided the root path to the raw input files in
	corpuspath argument or a preprocessed file path which contains each file
	in the form of a sentence. Each sentence is present at a separate line
	in the preprocessed file.

	To play with the parameters of word2vec like size of vectors find and change 
	the following line in the code:

        model = word2vec(contentpath=processedfilepath, min_count=5, size=500, sg=1, negative=5, iters=1,
                                         window=8, compute_loss=True, workers=20, alpha=0.025, batch_words=10000)

	min_count ====> minimum number of occurrences of the word in the corpus to be 
			considered by word2vec for creating its vector

	size =========> number of dimensions of the vector produced by word2vec
	
	sg ===========> this is always true as of now. (change later)

	negative =====> number of negative examples to be considered for negative sampling

	iters ========> number of iterations (i.e. number of times corpus is scanned for training)

	window =======> size of window used for scanning

	compute_loss => not implemented yet (leave as is)

	workers ======> how many processors to use in multiprocessing implementation

	alpha ========> learning rate

	batch_words ==> size of a batch



	If you want to explore the vectors created by this script you can use another
	script as follows:


	usage: explore_vecs.py [-h] -vecspath vecs_path

	optional arguments:
	  -h, --help            show this help message and exit
	  -vecspath vecs_path, --vecspath vecs_path
				which directory contains vectors

	



How does the output look like;;;

	The output consists of a pickle file which contains the input vectors
	in the form of a python dictionary.

